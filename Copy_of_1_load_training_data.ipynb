{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkp9Gnf00T0Z"
      },
      "source": [
        "# PREPARING MODEL TRAINING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXBhp0EB0T0a",
        "outputId": "bab412e0-595e-4ddc-fdd7-bab3ddf3f068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wordsegment\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wordsegment\n",
            "Successfully installed wordsegment-1.3.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wordsegment\n",
        "!pip install emoji\n",
        "# Initialise relevant packages\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Text cleaning\n",
        "from html import unescape\n",
        "import re\n",
        "import string\n",
        "import wordsegment as ws\n",
        "import emoji\n",
        "ws.load() # load vocab for word segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G81Q9Ton0T0b"
      },
      "source": [
        "## Load Raw Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw9T8QY60T0c"
      },
      "outputs": [],
      "source": [
        "# load raw data\n",
        "training_data = {}\n",
        "\n",
        "training_data['davidson2017'] = pd.read_csv('davidson2017.csv', index_col=0)\n",
        "training_data['founta2018'] = pd.read_csv('davidson2017.csv', names=['text', 'label', 'count_label_votes'], delimiter='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sujX5kSS0T0c"
      },
      "source": [
        "## Tidy Up Data Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKYDYA3q0T0c"
      },
      "outputs": [],
      "source": [
        "# specific formatting\n",
        "\n",
        "# Davidson 2017\n",
        "training_data['davidson2017'].rename(columns={\"class\": \"label\", \"tweet\": \"text\"}, inplace=True, errors='ignore')\n",
        "\n",
        "# Founta 2018\n",
        "# --> already fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq2uxj1d0T0c"
      },
      "outputs": [],
      "source": [
        "for dataset in training_data:\n",
        "\n",
        "    # create index column and rename to ID\n",
        "    training_data[dataset].reset_index(inplace=True)\n",
        "    training_data[dataset].rename(columns={'index': 'id'}, inplace=True, errors='ignore')\n",
        "\n",
        "    # drop unneccessary columns\n",
        "    training_data[dataset] = training_data[dataset][['id','text','label']]\n",
        "\n",
        "    # tidy up column types\n",
        "    training_data[dataset] = training_data[dataset].convert_dtypes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DybjuOQK0T0c"
      },
      "source": [
        "## Perform Basic Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azxokDBm0T0c"
      },
      "outputs": [],
      "source": [
        "# Define helper function for segmenting hashtags found through regex\n",
        "def regex_match_segmentation(match):\n",
        "    return ' '.join(ws.segment(match.group(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3uHle2S0T0c"
      },
      "outputs": [],
      "source": [
        "# Define function for cleaning text\n",
        "def clean_text(text):\n",
        "\n",
        "    # convert HTML codes\n",
        "    text = unescape(text)\n",
        "\n",
        "    # lowercase text\n",
        "    text = text.lower()\n",
        "\n",
        "    # replace mentions, URLs and emojis with special token\n",
        "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
        "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
        "    text = ''.join(' [EMOJI] ' if (char in emoji.UNICODE_EMOJI) else char for char in text).strip()\n",
        "\n",
        "    # find and split hashtags into words\n",
        "    text = re.sub(r\"#[A-Za-z0-9]+\", regex_match_segmentation, text)\n",
        "\n",
        "    # remove punctuation at beginning of string (quirk in Davidson data)\n",
        "    text = text.lstrip(\"!\")\n",
        "\n",
        "    # remove newline and tab characters\n",
        "    text = text.replace('\\n',' ')\n",
        "    text = text.replace('\\t',' ')\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ddnRot70T0d"
      },
      "outputs": [],
      "source": [
        "# apply text cleaner to text columns for each dataset\n",
        "import emoji\n",
        "\n",
        "def clean_text(text):\n",
        "    emoji_chars = set(emoji.EMOJI_DATA.keys())\n",
        "    text = ''.join(' [EMOJI] ' if char in emoji_chars else char for char in text).strip()\n",
        "    return text\n",
        "\n",
        "for dataset in training_data:\n",
        "    training_data[dataset]['text']=training_data[dataset].text.apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nQxWORi0T0d"
      },
      "source": [
        "## Export Multiclass Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuBMboq00T0d",
        "outputId": "420d4002-5d06-4992-882c-94d3a44ee4fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "davidson2017\n",
            "label\n",
            "hateful       1430\n",
            "neither       4163\n",
            "offensive    19190\n",
            "Name: id, dtype: Int64 \n",
            "\n",
            "founta2018\n",
            "Series([], Name: id, dtype: Int64) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-0134ef2cc6e5>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  training_data['davidson2017']['label'].replace({\n"
          ]
        }
      ],
      "source": [
        "# give multiclass labels string names for clarity\n",
        "# Davidson et al. (2017) --> 0 is \"hate speech\", 1 is \"offensive language\", 2 is \"neither\"\n",
        "#training_data['davidson2017'].label.replace({0: \"hateful\", 1: \"offensive\", 2: \"neither\"}, inplace = True)\n",
        "training_data['davidson2017']['label'] = training_data['davidson2017']['label'].astype(str) # Convert the 'label' column to string type\n",
        "training_data['davidson2017']['label'].replace({\n",
        "    '0': \"hateful\",\n",
        "    '1': \"offensive\",\n",
        "    '2': \"neither\"\n",
        "}, inplace = True)\n",
        "# print class frequencies for each dataset\n",
        "for dataset in training_data:\n",
        "    print(dataset)\n",
        "    print(training_data[dataset].groupby('label').id.count(), '\\n')\n",
        "\n",
        "# save dictionary of cleaned datasets to pickle\n",
        "pickle.dump(training_data, open('/content/training_data_multiclass.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788bI7AY0T0d"
      },
      "source": [
        "## Convert to Binary Classification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ycsx9nzD0T0d",
        "outputId": "a444f583-4ba4-417d-c864-96c1f48f5331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-938825ecc798>:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  training_data['davidson2017'].label.replace({'hateful': 1, 'offensive': 0, 'neither': 0}, inplace = True)\n",
            "<ipython-input-10-938825ecc798>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  training_data['founta2018'].label.replace({'hateful': 1, \"abusive\": 0, \"normal\": 0, \"spam\": 0}, inplace = True)\n"
          ]
        }
      ],
      "source": [
        "# GOAL: hateful (1) and non-hateful (0)\n",
        "\n",
        "# Davidson et al. (2017) --> \"hateful\", \"offensive\", \"neither\"\n",
        "training_data['davidson2017'].label.replace({'hateful': 1, 'offensive': 0, 'neither': 0}, inplace = True)\n",
        "\n",
        "# Founta et al. (2018) --> \"hateful\", \"abusive\", \"normal\", \"spam\"\n",
        "training_data['founta2018'].label.replace({'hateful': 1, \"abusive\": 0, \"normal\": 0, \"spam\": 0}, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhBLKpaB0T0d"
      },
      "source": [
        "## Export Binary Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmvsH4nB0T0d",
        "outputId": "e58e1344-7217-4fe8-ff3b-dfccb58e9811",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "davidson2017\n",
            "label\n",
            "0    23353\n",
            "1     1430\n",
            "Name: id, dtype: Int64 \n",
            "\n",
            "founta2018\n",
            "Series([], Name: id, dtype: Int64) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print class frequencies for each dataset\n",
        "for dataset in training_data:\n",
        "    print(dataset)\n",
        "    print(training_data[dataset].groupby('label').id.count(), '\\n')\n",
        "\n",
        "# save dictionary of cleaned datasets to pickle\n",
        "pickle.dump(training_data, open('/content/training_data_binary.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMBrS-Tb0T0e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}